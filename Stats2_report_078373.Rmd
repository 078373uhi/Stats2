---
title: "078373_Stats2_Report"
author: "Michelle Paterson"
date: "29/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
# Install packages
library(tidyverse)
library(tidytext)
install.packages("gutenbergr")
library(gutenbergr)
library(dplyr)
library(ggplot2)
library(wordcloud2)
library(viridisLite)
library(rvest)
library(patchwork)
library(stringr)
library(widyr)
library(wordcloud)
library(igraph)
library(ggraph)
library(reshape2)
library(tidygraph)
library(glue)
library(igraphdata)
library(visNetwork)
library(igraph)
```

```{r setup, include=FALSE}
# Read in data
navy_comms <- read.csv("navy_comms.csv")
navy_extra <- read.csv("navy_extra.csv")

# load in texts
battle <- read_lines("https://www.gutenberg.org/files/54441/54441-0.txt", skip = 214)
head(battle, 10)

battle_text <- tibble(source='Battle',text = battle)
head(battle_text)

navybook <- read_lines("https://www.gutenberg.org/cache/epub/41677/pg41677.txt", skip = 264)
head(navybook, 10)

navy_text <- tibble(source='Navy',text = navybook)
head(navy_text)

flag <- read_lines("https://www.gutenberg.org/cache/epub/19849/pg19849.txt", skip = 426)
head(flag, 10)

flag_text <- tibble(source='Flag',text = flag)
head(flag_text)

# collate the texts together into a corpus
BritNavy <- rbind(battle_text, navy_text, flag_text)

# tidy up the text 
tidy_BN <- BritNavy %>%
  unnest_tokens(word, text)

# remove stop words
tidy_BN <- tidy_BN %>%
  anti_join(stop_words)
```

```{r showBN, include=FALSE}
tidy_BN

```
## Main findings
### 6-8 bullet points

## Network Analysis

view(navy_comms)
view(navy_extra)


                                      
```{r comms, include=FALSE}
# edge dataframe
(edge_df <- data.frame(
  from = navy_comms$Source,
  to = navy_comms$Recipient
))

# create graph
(comms_graph <- graph_from_data_frame(edge_df, 
                                         directed = FALSE))

```                                      
                                      
## Sentiment Analysis
Three books were used for the sentiment analysis with the common theme of the British Navy.  These were The British Navy Book by Cyril Field, The British Navy in Battle by Arthur Joseph Hungerford Pollen and Flag and Fleet: How the British Navy Won the Freedom of the Seas by William Wood.

```{r countBN, echo=FALSE}
#count unique words then group by source (book)
tidy_BN %>%
  count(word, sort=TRUE)

tidy_BN_count <- tidy_BN %>%
  group_by(source) %>%
  count(word, sort=TRUE)

tidy_BN_count

```

### Word Count and Frequency

The British Navy Book is the book with the most words (41,909) followed by The British Navy in Battle (40,616) and Flag and Fleet has the least by quite a lot (32,834).  When looking at unique words used this is reflected too: The British Navy Book uses 9,901 unique words; The British Navy in Battle uses 7,182; and Flag and Fleet uses 6,803. 


```{r countbook}
#count words then group by source (book)
count(tidy_BN, source)

```

```{r countuniq}
#count words then group by source (book)
count(tidy_BN, source)

#count unique words per book

tidy_BN %>%
  filter(source == "Battle") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Navy") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Flag") %>%
  count(word, sort=TRUE)

```
This plot and wordcloud show the highest frequency words when the books are combined.

```{r plotBN}
tidy_BN %>%
  count(word, sort = TRUE) %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Word frequency for the three books together',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r cloudBN}
colour_pal <- turbo(n=100)

wc_data <- tidy_BN %>%
  count(word)  %>%
  filter(n > 100)   # only include words that appear more than a hundred times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```



```{r plotbattle, echo=FALSE}
plot_battle <- tidy_BN %>%
  filter(source == "Battle") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy in Battle',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_navy <- tidy_BN %>%
  filter(source == "Navy") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy Book',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_flag <- tidy_BN %>%
  filter(source == "Flag") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Flag and Fleet',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r countplots}
plot_battle + plot_navy + plot_flag
```

### Word importance weighting

```{r import}
# Show word importance by Tf-idf weighting for all books
tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  arrange(desc(tf_idf)) 
```

```{r importplot}
# Show word importance by Tf-idf weighting for each book
plot_importance <- tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  group_by(source) %>%
  top_n(10, tf_idf) %>%
  ungroup()
ggplot(plot_importance, aes(reorder(word, tf_idf), tf_idf, fill = source)) +
  geom_col(show.legend = FALSE) + labs(x = NULL, y = "tf-idf") +
  facet_wrap( ~ source, ncol = 5, scales = "free") + coord_flip()
```
### Word networks

```{r wordnetwork, echo=FALSE}
set.seed(1)

a <- grid::arrow(type='closed', length = unit(0.25,"cm"))

navy_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Navy") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy Book",
       x = "", y = "")

flag_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Flag") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "Flag and Fleet",
       x = "", y = "")

battle_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Battle") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy in Battle",
       x = "", y = "")
```

```{r netplots}
navy_net + battle_net + flag_net
```
```{r sent}
# add sentiments to previous text corpus
sent <- tidy_BN %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(source, word, sentiment, sort = TRUE) %>%
  ungroup()

head(sent)

sent %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(sentiment ~ source, nrow = 2, scales = "free_y") +
  labs(x = "Contribution to Sentiment",
       y = NULL) +
  theme(strip.text.x = element_text(margin = margin(2, 0, 2, 0)))
```
### Positivity measure

```{r positivity}
# Variation in overall positivity by book
# prepare text for analysis
tidy_BN2 <- tidy_BN %>%
    mutate(
    linenumber = row_number())

head(tidy_BN2)

# break text into chunks and measure positivity of each chunk
sentiment <- tidy_BN2 %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(source, index = linenumber %/% 1000, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
# index=linenumber %/% 1000, is splitting our text up into chunks of 1000 lines each.

head(sentiment)

# plot positivity by book
ggplot(sentiment, aes(index, sentiment, fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, scales = "free_x") +
  labs(x = "Positivity by book",
       y = "Scale of positive/negative") 
```



























```{r countnavy}
tidy_navy %>%
  count(word, sort=TRUE)
```
```{r plotnavy, echo=FALSE}
tidy_navy %>%
  count(word, sort = TRUE) %>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Word frequency for The British Navy Book',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```
```{r countflag}
tidy_flag %>%
  count(word, sort=TRUE)
```

```{r plotflag, echo=FALSE}
tidy_flag %>%
  count(word, sort = TRUE) %>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Word frequency for Flag and Fleet: How the British Navy Won the Freedom of the Seas',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r cloudbattle}
colour_pal <- turbo(n=100)

wc_data <- tidy_battle %>%
  count(word)  %>%
  filter(n > 10)   # only include words that appear more than ten times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```

```{r cloudnavy}
colour_pal <- turbo(n=100)

wc_data <- tidy_navy %>%
  count(word)  %>%
  filter(n > 10)   # only include words that appear more than ten times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```

```{r cloudflag}
colour_pal <- turbo(n=100)

wc_data <- tidy_flag %>%
  count(word)  %>%
  filter(n > 10)   # only include words that appear more than ten times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```


## Summary and Conclusion










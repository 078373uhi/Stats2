---
title: "078373_Stats2_Report"
author: "Michelle Paterson"
date: "29/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setuppack, include=FALSE}
# IF THERE IS A PROBLEM RUNNING THIS CHUNK IT APPEARS TO BE AN ISSUE WITH THE CHUNK ABOVE.  I HAVE NO IDEA WHY IT IS OCCURRING AS IT WORKED PERFECTLY FOR WEEKS WHILE I WORKED ON THIS, THEN THIS ISSUE BEGAN AT RANDOM. THE WAY I SOLVE IT IS TO OPEN A NEW R MARKDOWN FILE AND COPY AND PASTE THE SETUP CHUNK FROM THAT INTO THE ABOVE CHUNK THEN IT SEEMS TO WORK FINE.

# Install required packages
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(ggplot2)
library(wordcloud2)
library(viridisLite)
library(rvest)
library(patchwork)
library(stringr)
library(widyr)
library(wordcloud)
library(igraph)
library(ggraph)
library(reshape2)
library(tidygraph)
library(glue)
library(igraphdata)
library(visNetwork)
library(igraph)
```

```{r setupdata, include=FALSE}
# Read in data
navy_comms <- read.csv("navy_comms.csv")
navy_extra <- read.csv("navy_extra.csv")

# load in first text
battle <- read_lines("https://www.gutenberg.org/files/54441/54441-0.txt", skip = 214)
head(battle, 10)

# create a tibble
battle_text <- tibble(source='Battle',text = battle)
head(battle_text)

# load in second text
navybook <- read_lines("https://www.gutenberg.org/cache/epub/41677/pg41677.txt", skip = 264)
head(navybook, 10)

# create a tibble
navy_text <- tibble(source='Navy',text = navybook)
head(navy_text)

# load in third text
flag <- read_lines("https://www.gutenberg.org/cache/epub/19849/pg19849.txt", skip = 426)
head(flag, 10)

# create a tibble
flag_text <- tibble(source='Flag',text = flag)
head(flag_text)

# collate the texts together into a corpus
BritNavy <- rbind(battle_text, navy_text, flag_text)

# tidy up the text 
tidy_BN <- BritNavy %>%
  unnest_tokens(word, text)

# remove stop words
tidy_BN <- tidy_BN %>%
  anti_join(stop_words)
```

## Main findings

* Naval communications were inspected for network analysis and there were 193 communications between 20 vessels.
* The total number of communications to and from a vessel ranged from a maximum of 29 to a minimum of 5.
* Three books on the subject of the British Navy were selected for sentiment analysis.
* Top five words used when looking at the three books combined were British, ships, sea, war, and fleet.
* It was found that the sentiment of the books was largely negative though taking negation words into account could impact on that.

## Network Analysis

```{r com1, include=FALSE}
# join navycomms and navycomms extra to show additional information of type of asset
# and rename columns accordingly
navy_extra <- rename(navy_extra, Source = asset) #rename asset to Source

navy_source <- navy_comms %>%
  inner_join(navy_extra, by = "Source") #join tables based on Source column

navy_source <- rename(navy_source, SourceType = type) #rename type to SourceType
navy_source <- rename(navy_source, CommsType = Type) #rename type to CommsType

navy_source2 <- navy_source[, c(1, 5, 2, 3, 4)] #create table with reorder of columns

navy_extra2 <- rename(navy_extra, Recipient = Source) #rename Source to Recipient

navy_all<- navy_source2 %>%
  inner_join(navy_extra2, by = "Recipient") #join tables based on Recipient column

navy_all <- rename(navy_all, RecipientType = type) #rename type to RecipientType
navy_all <- navy_all[, c(1, 2, 3, 6, 4, 5)] #create table with reorder of columns
navy_all #view new table containing all navy comms data
```

The navy communications data contains 2 tables.  The first is a record of communications between naval resources.  There is a source and recipient of the communication as well as the type of communication (morse or radio) and the duration.  There are 193 entries in the table.  Duration of communication ranges from 0 to 8.5 minutes.  The second table lists each naval resource and what type of vessel it is. This shows that there are 20 different vessels included and there are 7 different types: aircraft carrier (5); battlecruiser (1); battleship (4); cruiser (4); escort carrier (2); repair ship (2); and submarine (2).  

```{r commsgraph, echo=FALSE}
# load the data
navy_edgelist <- navy_comms
navy_vertices <- navy_extra

# create directed graph object
navy_graph <- graph_from_data_frame(
  d = navy_edgelist,
  vertices = navy_vertices,
  directed = TRUE
)

navy_graph #view graph 

#create graph showing network links
ggraph(navy_graph, layout = 'kk') +
  geom_edge_fan(arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm')) +
  geom_node_point(size = 14, color = "pink") +
  geom_node_text(aes(label = name)) +
  labs(legend.position="none") +
  theme_void()


```         

There are 20 vertices and 193 edges.
```{r commsvert, echo=FALSE}
V(navy_graph) #show vertices

``` 

```{r commsedge, echo=FALSE}
E(navy_graph) #show edges

``` 

This is the number of unique other vessels that any particular vessel is communicating with, including outgoing, incoming and total communications.  HMS Ark Royal has the most incoming communications with 17 while HMS Albatross has the least with only 5.  HMS Unicorn made 19 outgoing communications; this was the most while HMS Eagle made the least with 4.
```{r commsdegree, echo=FALSE}
sort(degree(navy_graph, mode="in")) #show incoming communications
sort(degree(navy_graph, mode="out")) #show outgoing communications
sort(degree(navy_graph, mode="total")) #show total communications
``` 

When looking at the strength measurement it can be seen that  HMS Unicorn had the strongest network with the highest number of communications (29) while HMS ALbatross had the least communications (13).

```{r commsstrength, echo=FALSE}

sort(strength(navy_graph)) #show strength of communications in order of weakest to strongest
``` 

This measure of closeness shows that HMS Eagle has the highest centrality and therefore the least steps or distance of communication.  HMS Royal Oak and HMS Warsprite had the lowest centrality and are therefore less likely to hear news first in the fleet.

```{r commsclose, echo=FALSE}

sort(closeness(navy_graph, normalized=TRUE)) # show measure of closeness between nodes
``` 



```{r com, echo=FALSE, warning = FALSE}
# create plot of centrality
navy_tdy <- as_tbl_graph(navy_graph) #make graph into table

navy_tdy %>%  #create plot from table
  activate(nodes) %>%
  mutate(pagerank = centrality_pagerank()) %>%
  activate(edges) %>%
  mutate(betweenness = centrality_edge_betweenness()) %>%
  ggraph() +
  geom_edge_link(aes(alpha = betweenness)) +
  geom_node_point(aes(size = pagerank, colour = pagerank)) + 
  geom_node_text(aes(label = name), repel=TRUE) +
  # discrete colour legend
  scale_color_gradient(guide = 'legend') +
  labs(title = "Plot of centrality")

```

This network has just one component; it is made up of just one group of nodes.
```{r navycommunity, echo=FALSE}
components(navy_graph) # check components of network

set.seed(15)
navycomp <- components(navy_graph) #create graph view of network components

# create plot
ggraph(navy_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point(color = "lightblue",size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Plot of community level")

```  

Using walktrap to detect clusters shows 2 clusters with a modularity score of 0.091.  Changing the number of steps in the walk did not result in any better score than this.  
```{r navycommunity2, echo=FALSE}
# create walktrap decomposition to find clusters
navy <- decompose(navy_graph)[[1]]
components(navy)

cluster_walktrap(navy)

```        

### Network Level
The longest path between 2 nodes is 3 edges and can be seen below: HMS Eagle > HMS Activity > HMS Belfast > HMS Hermes.  The mean distance shows that the average number of edges between any two nodes in the network is 1.563 and the first five can be seen below as an example.
```{r measure, echo=FALSE}
diameter(navy_graph, directed=TRUE, weights=NA) #measure diameter of network
get_diameter(navy_graph, directed=TRUE, weights=NA) #show diameter 
mean_distance(navy_graph, directed=TRUE) # get mean distance between nodes
# show top five distance measures
distance <- distances(navy_graph, weights=NA)
distance[1:5,1:5]
```  

## Sentiment Analysis
Three books were used for the sentiment analysis with the common theme of the British Navy.  These were The British Navy Book by Cyril Field, The British Navy in Battle by Arthur Joseph Hungerford Pollen and Flag and Fleet: How the British Navy Won the Freedom of the Seas by William Wood.  The books were read in and any unnecessary introductory lines were removed.  The texts were then collated in a corpus and any stop words removed.  This allowed them to analysed.

```{r countBN, echo=FALSE}
#count unique words then group by source (book)
tidy_BN_count <- tidy_BN %>%
  group_by(source) %>%
  count(word, sort=TRUE)


```

### Word Count and Frequency

The British Navy Book has the most words (41,909) followed by The British Navy in Battle (40,616) while Flag and Fleet has the least (32,834).  This is reflected in the unique words used: The British Navy Book uses 9,901 unique words; The British Navy in Battle uses 7,182; and Flag and Fleet uses 6,803. 

```{r countuniq, echo=FALSE}
#count words then group by source (book)
count(tidy_BN, source)

#count unique words per book
tidy_BN %>%
  filter(source == "Battle") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Navy") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Flag") %>%
  count(word, sort=TRUE)

```
Due to the high word count in the corpus it was decided to choose words that appear more than 250 times in the word count.  The highest frequency words are what would be expected given the topic of the books: British; ships; sea; war; and fleet.

```{r plotBN, echo=FALSE}
# create plot of word frequency for corpus
tidy_BN %>%
  count(word, sort = TRUE) %>%
  filter(n > 250) %>% #filter to words used more than 250 times
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) + # create plot
  geom_col() +
  labs(title='Word frequency for the three books together',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r cloudBN, echo=FALSE}
colour_pal <- turbo(n=100)

# create word cloud showing top words in corpus
wc_data <- tidy_BN %>%
  count(word)  %>%
  filter(n > 100)   # only include words that appear more than a hundred times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```

When analysed as individual texts the results of the word counts showed many similarities.  The same words as previously feature highly along with other common words such as battle, German and gun/s.  As this was analysis of individual texts it was decided to feature words that occurred more than 100 times in a text.

```{r plotbattle, echo=FALSE}
#create plot of top words used in each book

plot_battle <- tidy_BN %>%
  filter(source == "Battle") %>% # filter by book 
  count(word, sort = TRUE) %>%
  filter(n > 100) %>% # include words used more than 100 times
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy in Battle',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_navy <- tidy_BN %>%
  filter(source == "Navy") %>% # filter by book 
  count(word, sort = TRUE) %>%
  filter(n > 100) %>% # include words used more than 100 times
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy Book',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_flag <- tidy_BN %>%
  filter(source == "Flag") %>% # filter by book 
  count(word, sort = TRUE) %>%
  filter(n > 100) %>% # include words used more than 100 times
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Flag and Fleet',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r countplots, echo=FALSE}
# show plots
plot_battle + plot_navy + plot_flag
```

### Word importance weighting

```{r import, echo=FALSE}
# Show word importance by Tf-idf weighting for all books
tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  arrange(desc(tf_idf)) 
```
When analysed for word importance it is seen that "enemy's" has by far the highest importance rating followed by "Koenigsberg" with these being the two most important words for The British Navy in Battle.  "Breech" was the most important word for The British Navy Book while or Flag and Fleet it was "Spaniards".

```{r importplot, echo=FALSE}
# Show word importance by Tf-idf weighting for each book in plots
plot_importance <- tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  group_by(source) %>%
  top_n(10, tf_idf) %>%
  ungroup()
ggplot(plot_importance, aes(reorder(word, tf_idf), tf_idf, fill = source)) +
  geom_col(show.legend = FALSE) + labs(x = NULL, y = "tf-idf") +
  facet_wrap( ~ source, ncol = 5, scales = "free") + coord_flip()
```

### Word networks

```{r wordnetwork, echo=FALSE}
set.seed(1) # set seed for reproducing

a <- grid::arrow(type='closed', length = unit(0.25,"cm"))

# create word network for Navy book
navy_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Navy") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy Book",
       x = "", y = "")

# create word network for Flag book
flag_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Flag") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "Flag and Fleet",
       x = "", y = "")

# create word network for Battle book
battle_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Battle") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy in Battle",
       x = "", y = "")
```

```{r netplots, echo=FALSE}
# show word networks
navy_net + battle_net + flag_net
```

### Positivity measures

It can be seen that when the text is allocated a positive or negative leaning, the corpus contains considerably  more negative words than positive.
```{r sentcorp, echo=FALSE}
# add sentiments to previous text corpus
sentcorp <- tidy_BN %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) 

# check data
head(sentcorp)

# create bar plot of positive vs negative for corpus
sentcorp %>%
  group_by(sentiment) %>%
  ggplot(aes(x = sentiment)) +
  geom_bar()

```

This is reflected when measuring the positivity of the individual books too.  The British Navy in Battle contains primarily negative language.  The British Navy Book similarly contains a high number of negative words though more positive than The British Navy in Battle.  Flag and Fleet has the highest number of positive words though these are still overtaken by a largely negative text.

```{r sent, echo=FALSE}
# add sentiments to previous text corpus
sent <- tidy_BN %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(source, word, sentiment, sort = TRUE) %>%
  ungroup()

# create plots of positive/negative words by book
sent %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(sentiment ~ source, nrow = 2, scales = "free_y") +
  labs(x = "Contribution to Sentiment",
       y = NULL) +
  theme(strip.text.x = element_text(margin = margin(2, 0, 2, 0)))
```
```{r positivity, echo=FALSE}
# Variation in overall positivity by book
# prepare text for analysis
tidy_BN2 <- tidy_BN %>%
    mutate(
    linenumber = row_number())

# break text into chunks and measure positivity of each chunk
sentiment <- tidy_BN2 %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(source, index = linenumber %/% 1000, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
# index=linenumber %/% 1000, is splitting our text up into chunks of 1000 lines each.

# plot positivity by book
ggplot(sentiment, aes(index, sentiment, fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, scales = "free_x") +
  labs(x = "Positivity by book",
       y = "Scale of positive/negative") 
```

#### Sentiment Context
It is important to consider the context of a word when looking at the positivity value and when negation words are added it can be seen that a number of words have been wrongly assessed as negative so it may be that the corpus is not as negative as it initially appears; however, the wrongly assessed positive words may negate some of this effect.
```{r sentcon, echo=FALSE}
# get our books and tidy them with bigrams
books_bigrams <- BritNavy %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# reformat to have words in sep. columns
bigrams_sep <- books_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stop words
bigrams_no_stop <- bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

# count bigrams
bigram_counts <- bigrams_no_stop %>%
  count(word1, word2, sort=TRUE)

# check which words have plotted in the wrong positive/negative direction
negation_words <- c("not", "no", "never", "without")

# count affected words
negated_words <- bigrams_sep %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

# plot affected words
negated_words %>%
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 12, with_ties = FALSE) %>%
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation") +
  ylab("Sentiment value by number of occurrences") +
  coord_flip()

```

## Executive Summary
In conclusion it can be seen that 193 communications between 20 naval vessels of 7 different types was analysed. Communications ranged between 0 and 8.5 minutes.  The total number of communications to and from a vessel ranged from 29 to just 5.  The highest number of incoming communications was 17 and the highest number of outgoings 19.  The lowest of these was 5 and 4 respectively.   HMS Unicorn had the strongest network and HMS Albatross had the weakest.  HMS Eagle had the highest centrality and while HMS Royal Oak and HMS Warsprite had the lowest centrality. The longest path between 2 nodes is 3 edges and the mean distance in the network is 1.563.

For the sentiment analysis three books on the subject of the British Navy were investigated.  The British Navy Book had the most words (41,909) while Flag and Fleet had the least with 32,834. Similarly, The British Navy Book used the most unique words (9,901) and Flag and Fleet used the least (6,803).  The top five words used when the books were combined into a corpus were  British, ships, sea, war, and fleet.  Individually they had similarly predictable common words such as battle, German and gun/s.  Both the combined corpus and the individual books showed primarily negative language when sentiment was considered though it was noted that the context of a word was important and considering negation words may revise this somewhat.











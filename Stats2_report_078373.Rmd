---
title: "078373_Stats2_Report"
author: "Michelle Paterson"
date: "29/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setuppack, include=FALSE}
# Install packages
library(tidyverse)
library(tidytext)
install.packages("gutenbergr")
library(gutenbergr)
library(dplyr)
library(ggplot2)
library(wordcloud2)
library(viridisLite)
library(rvest)
library(patchwork)
library(stringr)
library(widyr)
library(wordcloud)
library(igraph)
library(ggraph)
library(reshape2)
library(tidygraph)
library(glue)
library(igraphdata)
library(visNetwork)
library(igraph)
```

```{r setupdata, include=FALSE}
# Read in data
navy_comms <- read.csv("navy_comms.csv")
navy_extra <- read.csv("navy_extra.csv")

# load in texts
battle <- read_lines("https://www.gutenberg.org/files/54441/54441-0.txt", skip = 214)
head(battle, 10)

battle_text <- tibble(source='Battle',text = battle)
head(battle_text)

navybook <- read_lines("https://www.gutenberg.org/cache/epub/41677/pg41677.txt", skip = 264)
head(navybook, 10)

navy_text <- tibble(source='Navy',text = navybook)
head(navy_text)

flag <- read_lines("https://www.gutenberg.org/cache/epub/19849/pg19849.txt", skip = 426)
head(flag, 10)

flag_text <- tibble(source='Flag',text = flag)
head(flag_text)

# collate the texts together into a corpus
BritNavy <- rbind(battle_text, navy_text, flag_text)

# tidy up the text 
tidy_BN <- BritNavy %>%
  unnest_tokens(word, text)

# remove stop words
tidy_BN <- tidy_BN %>%
  anti_join(stop_words)
```

```{r showBN, include=FALSE}
tidy_BN

```
## Main findings

### 6-8 bullet points

## Network Analysis

This table shows the information used for the network analysis 

```{r com, include=FALSE}
# join navycomms and navycomms extra to show additional information of type of asset
# and rename columns accordingly
navy_extra <- rename(navy_extra, Source = asset)

navy_source <- navy_comms %>%
  inner_join(navy_extra, by = "Source")

navy_source <- rename(navy_source, SourceType = type)
navy_source <- rename(navy_source, CommsType = Type)

navy_source2 <- navy_source[, c(1, 5, 2, 3, 4)]

navy_extra2 <- rename(navy_extra, Recipient = Source)

navy_all<- navy_source2 %>%
  inner_join(navy_extra2, by = "Recipient")

navy_all <- rename(navy_all, RecipientType = type)
navy_all <- navy_all[, c(1, 2, 3, 6, 4, 5)]
navy_all
```

The navy communications data contains 2 tables.  The first is a record of communications between naval resources.  There is a source and recipient of the communication as well as the type of communication and the duration.  The second table lists each naval resource and what type of vessel it is.  

Looking at this graph of the navy communications data network it can be seen that this is a directed graph with named vertices.  It has 20 vertices and 193 edges.  This is a bipartite graph as it also contains an attribute that shows the type of communication; either morse or radio.
       
```{r commsgraph, include=TRUE}
# load the data
navy_edgelist <- navy_comms
navy_vertices <- navy_extra

# create undirected graph object
navy_graph <- graph_from_data_frame(
  d = navy_edgelist,
  vertices = navy_vertices,
  directed = TRUE
)

navy_graph

ggraph(navy_graph, layout = 'kk') +
  geom_edge_fan(arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm')) +
  geom_node_point(size = 14, color = "pink") +
  geom_node_text(aes(label = name)) +
  labs(legend.position="none") +
  theme_void()


```         

This is a list of vertices.
```{r commsvert, include=TRUE}
V(navy_graph)

``` 

This is a list of edges
```{r commsedge, include=TRUE}
E(navy_graph)

``` 

These graphs show the number of unique other vessels that a particular vessel is communicating with.
```{r commsdegree, include=TRUE}
sort(degree(navy_graph, mode="in"))
sort(degree(navy_graph, mode="out"))
sort(degree(navy_graph, mode="total"))
``` 

When looking at the strength measurement it can be seen that  HMS Unicorn had the strongest network with the highest number of communications (29) while HMS ALbatross had the least communications (13).

```{r commsstrength, include=TRUE}

sort(strength(navy_graph))
``` 

This measure of closeness shows that HMS Eagle has the highest centrality and therefore the least steps or distance of communication.  HMS Royal Oak and HMS Warsprite had the lowest centrality and are therefore less likely to hear news first in the fleet.

```{r commsclose, include=TRUE}

sort(closeness(navy_graph, normalized=TRUE))
``` 



```{r com, include=TRUE}

navy_tdy <- as_tbl_graph(navy_graph)

navy_tdy %>% 
  activate(nodes) %>%
  mutate(pagerank = centrality_pagerank()) %>%
  activate(edges) %>%
  mutate(betweenness = centrality_edge_betweenness()) %>%
  ggraph() +
  geom_edge_link(aes(alpha = betweenness)) +
  geom_node_point(aes(size = pagerank, colour = pagerank)) + 
  geom_node_text(aes(label = name), repel=TRUE) +
  # discrete colour legend
  scale_color_gradient(guide = 'legend') +
  labs(title = "Plot of centrality")

```

```{r navycommunity, include=TRUE}
components(navy_graph)

set.seed(15)
navycomp <- components(navy_graph)

ggraph(navy_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point(color = "lightblue",size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Plot of community level")

```  
     
```{r navycommunity2, include=TRUE}
navy <- decompose(navy_graph)[[1]]
components(navy)

cluster_walktrap(navy)

```        
    ###### start here next time#####   
                                      
```{r comms, include=TRUE}
# edge dataframe
(edge_df <- data.frame(
  from = navy_comms$Source,
  to = navy_comms$Recipient
))

# create graph
(comms_graph <- graph_from_data_frame(edge_df, 
                                         directed = FALSE))

```                                      
 
 
 
 
                                      

```{r comms2, echo=TRUE}
# set seed for reproducibility
set.seed(1)

# create random layout (there are other layouts you can try)
l <- layout_randomly(comms_graph)

# plot with random layout
plot(comms_graph, layout = l)


# circle layout
set.seed(2)
round <- layout_in_circle(comms_graph)
plot(comms_graph, layout = round)


# F-R algorithm
set.seed(3)
fr <- layout_with_fr(comms_graph)
plot(comms_graph, layout = fr)

# K-K algorithm
set.seed(4)
kk <- layout_with_kk(comms_graph)
plot(comms_graph, layout = kk)

# set seed for reproducibility
set.seed(5)

# visualise using ggraph with fr layout
ggraph(comms_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() 
```


## Sentiment Analysis
Three books were used for the sentiment analysis with the common theme of the British Navy.  These were The British Navy Book by Cyril Field, The British Navy in Battle by Arthur Joseph Hungerford Pollen and Flag and Fleet: How the British Navy Won the Freedom of the Seas by William Wood.  The books were read in and any unnecessary introductory lines were removed.  The texts were then collated in a corpus and any stop words removed.  This allowed them to analysed.

```{r countBN, echo=FALSE}
#count unique words then group by source (book)
tidy_BN_count <- tidy_BN %>%
  group_by(source) %>%
  count(word, sort=TRUE)


```

### Word Count and Frequency

The British Navy Book is the book with the most words (41,909) followed by The British Navy in Battle (40,616) and Flag and Fleet has the least by quite an amount (32,834).  When looking at unique words used this is reflected too: The British Navy Book uses 9,901 unique words; The British Navy in Battle uses 7,182; and Flag and Fleet uses 6,803. 

```{r countuniq, echo=FALSE}
#count words then group by source (book)
count(tidy_BN, source)

#count unique words per book

tidy_BN %>%
  filter(source == "Battle") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Navy") %>%
  count(word, sort=TRUE)

tidy_BN %>%
  filter(source == "Flag") %>%
  count(word, sort=TRUE)

```
This plot and wordcloud show the highest frequency words when the books are combined.  Due to the high word count in the corpus it was decided to choose words that appear more than 250 times in the word count.  The highest frequency words are what would be expected given the topic of the books: British; ships; sea; war; and fleet.

```{r plotBN, echo=FALSE}
tidy_BN %>%
  count(word, sort = TRUE) %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Word frequency for the three books together',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r cloudBN, echo=FALSE}
colour_pal <- turbo(n=100)

wc_data <- tidy_BN %>%
  count(word)  %>%
  filter(n > 100)   # only include words that appear more than a hundred times
wordcloud2(wc_data, shape='circle', size=0.5, color=colour_pal)
```

When analysed as individual texts the results of the word counts showed many similarities.  The same words as previously feature highly along with other common words such as battle, German and gun/s.  As this was analysis of individual texts it was decided to feature words that occurred more than 100 times in a text.

```{r plotbattle, echo=FALSE}
plot_battle <- tidy_BN %>%
  filter(source == "Battle") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy in Battle',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_navy <- tidy_BN %>%
  filter(source == "Navy") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='The British Navy Book',
       y = NULL, x= "word count") +
  theme(legend.position="none")

plot_flag <- tidy_BN %>%
  filter(source == "Flag") %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill=word)) +
  geom_col() +
  labs(title='Flag and Fleet',
       y = NULL, x= "word count") +
  theme(legend.position="none")
```

```{r countplots, echo=FALSE}
plot_battle + plot_navy + plot_flag
```

### Word importance weighting

```{r import, echo=FALSE}
# Show word importance by Tf-idf weighting for all books
tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  arrange(desc(tf_idf)) 
```
When analysed for word importance it is seen that "enemy's" has by far the highest importance rating followed by "Koenigsberg" with these being the two most important words for The British Navy in Battle.  "Breech" was the most important word for The British Navy Book while or Flag and Fleet it was "Spaniards".

```{r importplot, echo=FALSE}
# Show word importance by Tf-idf weighting for each book
plot_importance <- tidy_BN_count %>%
  bind_tf_idf(word, source, n) %>%
  group_by(source) %>%
  top_n(10, tf_idf) %>%
  ungroup()
ggplot(plot_importance, aes(reorder(word, tf_idf), tf_idf, fill = source)) +
  geom_col(show.legend = FALSE) + labs(x = NULL, y = "tf-idf") +
  facet_wrap( ~ source, ncol = 5, scales = "free") + coord_flip()
```

### Word networks

```{r wordnetwork, echo=FALSE}
set.seed(1)

a <- grid::arrow(type='closed', length = unit(0.25,"cm"))

navy_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Navy") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy Book",
       x = "", y = "")

flag_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Flag") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "Flag and Fleet",
       x = "", y = "")

battle_net <- tidy_BN_count %>%
  filter(n>150) %>%
  filter(source == "Battle") %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(),
                 arrow = a, end_cap = circle(0.25, "cm")) +
  geom_node_point(colour="lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme(legend.position="none")+
  theme_void() +
  labs(title = "Word Network",
       subtitle = "The British Navy in Battle",
       x = "", y = "")
```

```{r netplots, echo=FALSE}
navy_net + battle_net + flag_net
```

### Positivity measures

It can be seen that when the text is allocated a positive or negative leaning, the corpus contains considerably  more negative words than positive.
```{r sentcorp, echo=FALSE}
# add sentiments to previous text corpus
sentcorp <- tidy_BN %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) 

head(sentcorp)

sentcorp %>%
  group_by(sentiment) %>%
  ggplot(aes(x = sentiment)) +
  geom_bar()

```

This is reflected when measuring the positivity of the individual books too.  The British Navy in Battle contains primarily negative language.  The British Navy Book similarly contains a high number of negative words though more positive than The British Navy in Battle.  Flag and Fleet has the highest number of positive words though these are still overtaken by a largely negative text.

```{r sent, echo=FALSE}
# add sentiments to previous text corpus
sent <- tidy_BN %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(source, word, sentiment, sort = TRUE) %>%
  ungroup()

sent %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(sentiment ~ source, nrow = 2, scales = "free_y") +
  labs(x = "Contribution to Sentiment",
       y = NULL) +
  theme(strip.text.x = element_text(margin = margin(2, 0, 2, 0)))
```
```{r positivity, echo=FALSE}
# Variation in overall positivity by book
# prepare text for analysis
tidy_BN2 <- tidy_BN %>%
    mutate(
    linenumber = row_number())

# break text into chunks and measure positivity of each chunk
sentiment <- tidy_BN2 %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(source, index = linenumber %/% 1000, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
# index=linenumber %/% 1000, is splitting our text up into chunks of 1000 lines each.

# plot positivity by book
ggplot(sentiment, aes(index, sentiment, fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, scales = "free_x") +
  labs(x = "Positivity by book",
       y = "Scale of positive/negative") 
```

#### Sentiment Context
It is important to consider the context of a word when looking at the positivity value and when negation words are added it can be seen that a number of words have been wrongly assessed as negative so it may be that the corpus is not as negative as it initially appears; however, the wrongly assessed positive words may negate some of this effect.
```{r sentcon, echo=FALSE}
# get our books and tidy them with bigrams
books_bigrams <- BritNavy %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# reformat to have words in sep. columns
bigrams_sep <- books_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stop words
bigrams_no_stop <- bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

# count bigrams
bigram_counts <- bigrams_no_stop %>%
  count(word1, word2, sort=TRUE)

# check which words have plotted in the wrong positive/negative direction
negation_words <- c("not", "no", "never", "without")

# count affected words
negated_words <- bigrams_sep %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

# plot affected words
negated_words %>%
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 12, with_ties = FALSE) %>%
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation") +
  ylab("Sentiment value by number of occurrences") +
  coord_flip()

```
## Summary and Conclusion









